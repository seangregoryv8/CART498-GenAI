# -*- coding: utf-8 -*-
"""PxScript.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B7bA9lMlm7p-t1hBps1EeBscVqrcKUd7
"""

# Tokenization

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")
model.eval()

prompt = "One must have a mind of winter \
# To regard the frost and the boughs \
# Of the pine-trees crusted with snow; \
# And have been cold a long time \
# To behold the junipers shagged with ice, \
# The spruces rough in the distant glitter \
# Of the January sun; and not to think \
# Of any misery in the sound of the wind, \
# In the sound of a few leaves, \
# Which is the sound of the land \
# Full of the same wind \
# That is blowing in the same bare place \
# For the listener, who listens in the snow, \
# And, nothing himself, beholds \
# Nothing that is not there and the nothing that is. "

def get_p_x_word(context, x=7):
    inputs = tokenizer(context, return_tensors="pt")

    with torch.no_grad():
        outputs = model(**inputs)

    # Get logits for the next token
    logits = outputs.logits[0, -1]
    probs = torch.softmax(logits, dim=0)

    # Get top candidates
    top_probs, top_indices = torch.topk(probs, k=50)

    # Decode tokens
    tokens = [tokenizer.decode([idx]).strip() for idx in top_indices]

    # Return the x-th token (1-based index)
    return tokens[x-1]

import re

# sentences = re.split(r'(?<=[.!?])\s', prompt)
sentences = re.split(r'#', prompt)

print("Sentences detected:")
for i, sentence in enumerate(sentences):
    cleaned_sentence = sentence.strip()
    print(f"\n--- Sentence {i+1} ---")
    print(cleaned_sentence)
    print("\n")
    sentence = cleaned_sentence
    words = sentence.split(" ");
    new_words = words[0:-1];

    for word in new_words:
        print(word);
    # print(words);
    # print(words[-1]);

processed_lines = []

for sentence in sentences:
    sentence = sentence.strip()
    if not sentence:
        continue

    words = sentence.split(" ")
    if len(words) < 2:
        processed_lines.append(sentence)
        continue

    context = " ".join(words[:-1])
    new_word = get_p_x_word(context, x=7)

    new_line = context + " " + new_word
    processed_lines.append(new_line)

with open("P+7.txt", "w") as f:
    for line in processed_lines:
        print(line + "\n")
        f.write(line + "\n")

processed_lines_px = []

for sentence in sentences:
    sentence = sentence.strip()
    if not sentence:
        continue

    words = sentence.split(" ")
    context = " ".join(words[:-1])
    new_word = get_p_x_word(context, x=21)  # Switch this for any Px value you wnat
    processed_lines_px.append(context + " " + new_word)

with open("P+40.txt", "w") as f:
    for line in processed_lines_px:
        print(line + "\n")
        f.write(line + "\n")

import re

# The original prompt string after Python processes the backslashes
# is a single continuous string. The '#' characters are part of the string content.

# Split the prompt into sentences based on the '#' character, as per user's intent
sentences = prompt.split('#')

print("Sentences detected:")
for i, sentence in enumerate(sentences):
    # Clean up the sentence by removing leading/trailing whitespace
    cleaned_sentence = sentence.strip()

    # Skip empty sentences that might result from splitting (e.g., if prompt ends with '#')
    if not cleaned_sentence:
        continue

    print(f"\n--- Sentence {i+1} ---")
    print(f"Original sentence part: {sentence}")
    print(f"Cleaned sentence part: {cleaned_sentence}")

    # Tokenize each sentence
    sentence_input_ids = tokenizer(cleaned_sentence).input_ids
    print(f"Tokens for sentence {i+1}:")
    for t in sentence_input_ids:
        print(f"{t}\t: {tokenizer.decode(t)}")

"""# Task
Load a `GPT2LMHeadModel` from the `transformers` library. Then, modify the code in cell `qXLwPfX7ok9w` to iterate through each `cleaned_sentence`, use the loaded language model to predict a sequence of next tokens, and print the 7th predicted token from this sequence instead of `words[-1]`.

## Load Language Model

### Subtask:
Load a pre-trained language model (e.g., `GPT2LMHeadModel`) from the `transformers` library.

**Reasoning**:
The subtask requires loading a pre-trained language model. This step imports the necessary class and loads the 'gpt2' model.
"""

from transformers import GPT2LMHeadModel

model = GPT2LMHeadModel.from_pretrained("gpt2")
print("GPT2LMHeadModel loaded successfully.")

"""**Reasoning**:
The previous model loading step was successful, indicated by 'GPT2LMHeadModel loaded successfully.' in the output, despite the warnings and download progress. The next step is to modify cell `qXLwPfX7ok9w` to use the loaded language model for token prediction as per the task instructions.


"""

import re
import torch # Import torch for tensor operations

# sentences = re.split(r'(?<=[.!?])\s', prompt)
sentences = re.split(r'#', prompt)

print("Sentences detected:")
for i, sentence in enumerate(sentences):
    cleaned_sentence = sentence.strip()

    # Skip empty sentences that might result from splitting
    if not cleaned_sentence:
        continue

    print(f"\n--- Sentence {i+1} ---")
    print(cleaned_sentence)

    # Tokenize the cleaned sentence
    input_ids = tokenizer.encode(cleaned_sentence, return_tensors='pt')

    # Ensure model is in evaluation mode
    model.eval()

    # Generate a sequence of tokens. We want 7 *new* tokens.
    output_sequences = model.generate(
        input_ids=input_ids,
        max_new_tokens=7, # Generate up to 7 new tokens
        do_sample=False,  # Use greedy decoding for deterministic results
        num_beams=1,      # Required for greedy decoding with do_sample=False
        pad_token_id=tokenizer.eos_token_id # Prevents errors if tokenizer doesn't have a pad_token_id
    )

    # The generated sequence includes the input tokens.
    # We need the 7th *new* token, which is at index (input_ids.shape[1] + 6) (0-indexed).
    if output_sequences.shape[1] >= input_ids.shape[1] + 7:
        seventh_predicted_token_id = output_sequences[0, input_ids.shape[1] + 6].item()
        seventh_predicted_token_str = tokenizer.decode(seventh_predicted_token_id, skip_special_tokens=True)
        print(f"7th predicted token: '{seventh_predicted_token_str}'")
    else:
        print("Less than 7 new tokens were predicted after the input sentence.")
        # Optionally, print all predicted new tokens if less than 7 were generated
        predicted_new_tokens_ids = output_sequences[0, input_ids.shape[1]:]
        print(f"All predicted new tokens: '{tokenizer.decode(predicted_new_tokens_ids, skip_special_tokens=True)}'")
